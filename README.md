#!/usr/bin/env python3
"""
áƒáƒ áƒáƒ›áƒ”áƒ¢áƒ” - áƒ£áƒœáƒ˜áƒ•áƒ”áƒ áƒ¡áƒáƒšáƒ£áƒ áƒ˜ áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜ AI áƒáƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒœáƒ¢áƒ˜
áƒáƒ•áƒ¢áƒáƒ›áƒáƒ¢áƒ£áƒ áƒáƒ“ áƒáƒ áƒ’áƒ”áƒ‘áƒ¡ áƒ›áƒáƒ“áƒ”áƒšáƒ¡ áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ˜áƒ¡ áƒ áƒ”áƒ¡áƒ£áƒ áƒ¡áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ˜áƒ®áƒ”áƒ“áƒ•áƒ˜áƒ—
"""

import os
import sys
import json
import torch
import psutil
import platform
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Union
import argparse
import warnings
warnings.filterwarnings("ignore")

# ==================== áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ˜áƒ¡ áƒáƒ¦áƒ›áƒáƒ©áƒ”áƒœáƒ ====================
class SystemDetector:
    @staticmethod
    def get_system_info() -> Dict:
        """áƒáƒ•áƒ¢áƒáƒ›áƒáƒ¢áƒ£áƒ áƒáƒ“ áƒáƒ›áƒáƒ˜áƒªáƒœáƒáƒ‘áƒ¡ áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ˜áƒ¡ áƒ áƒ”áƒ¡áƒ£áƒ áƒ¡áƒ”áƒ‘áƒ¡"""
        info = {
            "os": platform.system(),
            "os_version": platform.version(),
            "architecture": platform.architecture()[0],
            "processor": platform.processor(),
            "ram_total_gb": psutil.virtual_memory().total / (1024**3),
            "ram_available_gb": psutil.virtual_memory().available / (1024**3),
            "disk_free_gb": psutil.disk_usage('/').free / (1024**3) if os.name != 'nt' else psutil.disk_usage('C:').free / (1024**3),
            "is_android": 'android' in platform.system().lower() or 'ANDROID_ROOT' in os.environ,
            "is_mobile": platform.system() in ['Android', 'iOS', 'Darwin'] and 'Mobile' in platform.platform(),
            "has_gpu": torch.cuda.is_available() if torch else False,
            "cpu_cores": psutil.cpu_count(logical=False),
            "threads": psutil.cpu_count(logical=True)
        }
        
        # GPU áƒ˜áƒœáƒ¤áƒ
        if info["has_gpu"]:
            info["gpu_name"] = torch.cuda.get_device_name(0)
            info["gpu_memory_gb"] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        else:
            info["gpu_name"] = "None"
            info["gpu_memory_gb"] = 0
            
        return info
    
    @staticmethod
    def recommend_model(system_info: Dict) -> str:
        """áƒ áƒ”áƒ™áƒáƒ›áƒ”áƒœáƒ“áƒáƒªáƒ˜áƒ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¨áƒ”áƒ¡áƒáƒ®áƒ”áƒ‘ áƒ áƒ”áƒ¡áƒ£áƒ áƒ¡áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ˜áƒ®áƒ”áƒ“áƒ•áƒ˜áƒ—"""
        available_ram = system_info["ram_available_gb"]
        
        if available_ram < 1:
            return "error"  # áƒ«áƒáƒšáƒ˜áƒáƒœ áƒªáƒáƒ¢áƒ RAM
        elif available_ram <= 2:
            return "micro"   # 1-2GB RAM
        elif available_ram <= 4:
            return "tiny"    # 2-4GB RAM
        elif available_ram <= 8:
            return "base"    # 4-8GB RAM
        elif available_ram <= 16:
            return "standard" # 8-16GB RAM
        elif available_ram <= 32:
            return "pro"     # 16-32GB RAM
        else:
            return "ultra"   # 32GB+ RAM

# ==================== áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒ”áƒœáƒ”áƒ¯áƒ”áƒ áƒ˜ ====================
class ModelManager:
    MODEL_CONFIGS = {
        "micro": {
            "name": "prometheus-micro",
            "base_model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "quantization": "q4_0",
            "ram_required": 1.5,
            "storage_required": 0.5,
            "features": ["chat", "qa", "translation_basic", "summarization"],
            "languages": ["ka", "en"],
            "optimized_for": ["android", "low-end-pc", "raspberry-pi"]
        },
        "tiny": {
            "name": "prometheus-tiny",
            "base_model": "microsoft/phi-2",
            "quantization": "q4_K_M",
            "ram_required": 2.5,
            "storage_required": 1.2,
            "features": ["chat", "qa", "translation", "summarization", "sentiment", "entities"],
            "languages": ["ka", "en", "ru"],
            "optimized_for": ["android", "pc", "server"]
        },
        "base": {
            "name": "prometheus-base",
            "base_model": "Qwen/Qwen2.5-1.5B-Instruct",
            "quantization": "q5_K_M",
            "ram_required": 4.0,
            "storage_required": 2.0,
            "features": ["chat", "qa", "translation", "summarization", "code", "reasoning"],
            "languages": ["ka", "en", "ru", "tr"],
            "optimized_for": ["pc", "server", "web"]
        },
        "standard": {
            "name": "prometheus-standard",
            "base_model": "google/gemma-2b-it",
            "quantization": "q6_K",
            "ram_required": 6.0,
            "storage_required": 3.0,
            "features": ["chat", "qa", "translation", "code", "reasoning", "creative"],
            "languages": ["ka", "en", "ru", "tr", "az"],
            "optimized_for": ["pc", "server", "web", "api"]
        },
        "pro": {
            "name": "prometheus-pro",
            "base_model": "mistralai/Mistral-7B-Instruct-v0.2",
            "quantization": "q8_0",
            "ram_required": 12.0,
            "storage_required": 7.0,
            "features": ["all", "multimodal", "advanced_analysis", "long_context"],
            "languages": ["ka", "en", "ru", "tr", "az", "de", "fr"],
            "optimized_for": ["workstation", "server", "cloud"]
        },
        "ultra": {
            "name": "prometheus-ultra",
            "base_model": "deepseek-ai/deepseek-llm-7b-chat",
            "quantization": "none",
            "ram_required": 24.0,
            "storage_required": 14.0,
            "features": ["all", "multimodal", "advanced_reasoning", "research"],
            "languages": ["all_supported"],
            "optimized_for": ["server", "cloud", "enterprise"]
        }
    }
    
    @staticmethod
    def download_model(model_type: str, force_download: bool = False):
        """áƒáƒ•áƒ¢áƒáƒ›áƒáƒ¢áƒ£áƒ áƒ˜ áƒ©áƒáƒ›áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ"""
        config = ModelManager.MODEL_CONFIGS[model_type]
        model_path = Path(f"./models/{config['name']}")
        
        if model_path.exists() and not force_download:
            print(f"âœ… áƒ›áƒáƒ“áƒ”áƒšáƒ˜ '{config['name']}' áƒ£áƒ™áƒ•áƒ” áƒáƒ áƒ¡áƒ”áƒ‘áƒáƒ‘áƒ¡")
            return str(model_path)
        
        print(f"ğŸ“¥ áƒ©áƒáƒ›áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ: {config['name']}...")
        
        # áƒáƒ¥ áƒ˜áƒ¥áƒœáƒ”áƒ‘áƒ áƒ©áƒáƒ›áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ˜áƒ¡ áƒšáƒáƒ’áƒ˜áƒ™áƒ Hugging Face-áƒ“áƒáƒœ
        # áƒ¢áƒ”áƒ›áƒáƒáƒ áƒáƒ áƒ£áƒšáƒáƒ“ áƒ•áƒ¥áƒ›áƒœáƒ˜áƒ— dummy áƒ›áƒáƒ“áƒ”áƒšáƒ¡
        model_path.mkdir(parents=True, exist_ok=True)
        
        # áƒ¨áƒ”áƒ•áƒ¥áƒ›áƒœáƒáƒ— áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ¤áƒáƒ˜áƒšáƒ˜
        config_file = model_path / "config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
            
        # áƒ¨áƒ”áƒ•áƒ¥áƒ›áƒœáƒáƒ— dummy áƒ›áƒáƒ“áƒ”áƒšáƒ˜
        dummy_model = model_path / "model.bin"
        dummy_model.write_bytes(b'dummy_model_data')
        
        print(f"âœ… áƒ›áƒáƒ“áƒ”áƒšáƒ˜ '{config['name']}' áƒ›áƒ–áƒáƒ“áƒáƒ")
        return str(model_path)

# ==================== AI áƒ«áƒ áƒáƒ•áƒ ====================
class PrometheusEngine:
    def __init__(self, model_type: str = "auto"):
        self.system_info = SystemDetector.get_system_info()
        
        if model_type == "auto":
            self.model_type = SystemDetector.recommend_model(self.system_info)
        else:
            self.model_type = model_type
            
        if self.model_type == "error":
            raise SystemError("âŒ áƒáƒ áƒáƒ¡áƒáƒ™áƒ›áƒáƒ áƒ˜áƒ¡áƒ˜ RAM! áƒ¡áƒáƒ­áƒ˜áƒ áƒáƒ áƒ›áƒ˜áƒœáƒ˜áƒ›áƒ£áƒ› 1GB.")
            
        self.config = ModelManager.MODEL_CONFIGS[self.model_type]
        self.model_path = ModelManager.download_model(self.model_type)
        self.model = None
        self.tokenizer = None
        
        print(f"\n{'='*50}")
        print(f"ğŸ¤– áƒáƒ áƒáƒ›áƒ”áƒ¢áƒ” AI - {self.config['name']}")
        print(f"ğŸ“Š áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ: {self.system_info['os']} | RAM: {self.system_info['ram_available_gb']:.1f}GB")
        print(f"ğŸ¯ áƒ›áƒáƒ“áƒ”áƒšáƒ˜: {self.model_type} | áƒ”áƒœáƒ”áƒ‘áƒ˜: {', '.join(self.config['languages'])}")
        print(f"âš¡ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ”áƒ‘áƒ˜: {', '.join(self.config['features'][:5])}")
        print(f"{'='*50}\n")
        
        self._load_model()
    
    def _load_model(self):
        """áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ"""
        print(f"ğŸ”„ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ...")
        
        try:
            # áƒáƒ¥ áƒ˜áƒ¥áƒœáƒ”áƒ‘áƒ áƒ áƒ”áƒáƒšáƒ£áƒ áƒ˜ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ
            # áƒ¢áƒ”áƒ›áƒáƒáƒ áƒáƒ áƒ£áƒšáƒáƒ“ áƒ•áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ— áƒ›áƒáƒ áƒ¢áƒ˜áƒ• áƒšáƒáƒ’áƒ˜áƒ™áƒáƒ¡
            self.model = {"type": "dummy", "config": self.config}
            self.tokenizer = {"type": "dummy"}
            
            # áƒ™áƒ•áƒáƒœáƒ¢áƒ˜áƒ–áƒáƒªáƒ˜áƒ˜áƒ¡ áƒáƒ áƒ©áƒ”áƒ•áƒ
            if self.config["quantization"] != "none":
                print(f"ğŸ”§ áƒ™áƒ•áƒáƒœáƒ¢áƒ˜áƒ–áƒáƒªáƒ˜áƒ: {self.config['quantization']}")
            
            print(f"âœ… áƒ›áƒáƒ“áƒ”áƒšáƒ˜ áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ!")
            
        except Exception as e:
            print(f"âŒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {e}")
            self._fallback_to_lightweight()
    
    def _fallback_to_lightweight(self):
        """áƒ¡áƒáƒ­áƒ˜áƒ áƒáƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒáƒ¨áƒ˜ áƒ£áƒ¤áƒ áƒ áƒ›áƒ¡áƒ£áƒ‘áƒ£áƒ¥ áƒ›áƒáƒ“áƒ”áƒšáƒ–áƒ” áƒ’áƒáƒ“áƒáƒ¡áƒ•áƒšáƒ"""
        print("ğŸ”„ áƒ£áƒ¤áƒ áƒ áƒ›áƒ¡áƒ£áƒ‘áƒ£áƒ¥ áƒ›áƒáƒ“áƒ”áƒšáƒ–áƒ” áƒ’áƒáƒ“áƒáƒ¡áƒ•áƒšáƒ...")
        model_types = ["ultra", "pro", "standard", "base", "tiny", "micro"]
        
        for mt in model_types:
            if mt == self.model_type:
                continue
                
            req_ram = ModelManager.MODEL_CONFIGS[mt]["ram_required"]
            if self.system_info["ram_available_gb"] >= req_ram:
                self.model_type = mt
                self.config = ModelManager.MODEL_CONFIGS[mt]
                self.model_path = ModelManager.download_model(mt)
                print(f"âœ… áƒ’áƒáƒ“áƒáƒ•áƒ”áƒ“áƒ˜áƒ—: {self.config['name']}")
                break
    
    def process(self, prompt: str, language: str = "auto") -> str:
        """áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ"""
        if language == "auto":
            # áƒ”áƒœáƒ˜áƒ¡ áƒáƒ•áƒ¢áƒ-áƒ’áƒáƒ›áƒáƒªáƒœáƒáƒ‘áƒ
            if any(char in prompt for char in "áƒáƒ‘áƒ’áƒ“áƒ”áƒ•áƒ–áƒ—áƒ˜áƒ™áƒšáƒ›áƒœáƒáƒáƒŸáƒ áƒ¡áƒ¢áƒ£áƒ¤áƒ¥áƒ¦áƒ§áƒ¨áƒ©áƒªáƒ«áƒ¬áƒ­áƒ®áƒ¯áƒ°"):
                language = "ka"
            else:
                language = "en"
        
        # áƒ¡áƒ˜áƒ›áƒ£áƒšáƒáƒªáƒ˜áƒ£áƒ áƒ˜ áƒáƒáƒ¡áƒ£áƒ®áƒ˜
        responses = {
            "ka": [
                f"áƒ’áƒáƒ›áƒáƒ áƒ¯áƒáƒ‘áƒ! áƒ›áƒ” áƒ•áƒáƒ  áƒáƒ áƒáƒ›áƒ”áƒ¢áƒ” ({self.config['name']}).\n\náƒ¨áƒ”áƒœáƒ˜ áƒ¨áƒ”áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ: '{prompt}'\n\náƒ›áƒ” áƒ¨áƒ”áƒ›áƒ˜áƒ«áƒšáƒ˜áƒ áƒ“áƒáƒ’áƒ”áƒ®áƒ›áƒáƒ áƒ: {', '.join(self.config['features'])}.",
                f"áƒáƒ áƒáƒ›áƒ”áƒ¢áƒ” áƒáƒ¥ áƒáƒ áƒ˜áƒ¡! áƒ áƒ”áƒŸáƒ˜áƒ›áƒ˜: {self.model_type}\n\náƒ¨áƒ”áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ: {prompt}\n\náƒáƒáƒ¡áƒ£áƒ®áƒ˜: áƒ”áƒ¡ áƒáƒ áƒ˜áƒ¡ áƒ“áƒ”áƒ›áƒ áƒáƒáƒ¡áƒ£áƒ®áƒ˜ {self.config['name']} áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ“áƒáƒœ."
            ],
            "en": [
                f"Hello! I'm Prometheus ({self.config['name']}).\n\nYour question: '{prompt}'\n\nI can help with: {', '.join(self.config['features'])}.",
                f"Prometheus here! Mode: {self.model_type}\n\nQuestion: {prompt}\n\nAnswer: This is a demo response from {self.config['name']} model."
            ]
        }
        
        import random
        return random.choice(responses.get(language, responses["en"]))
    
    def batch_process(self, prompts: List[str]) -> List[str]:
        """áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜áƒ›áƒ” áƒ¨áƒ”áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ˜áƒ¡ áƒ”áƒ áƒ—áƒ“áƒ áƒáƒ£áƒšáƒáƒ“ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ"""
        return [self.process(p) for p in prompts]
    
    def get_capabilities(self) -> Dict:
        """áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒ”áƒ‘áƒšáƒáƒ‘áƒ”áƒ‘áƒ˜"""
        return {
            "model": self.config["name"],
            "type": self.model_type,
            "features": self.config["features"],
            "languages": self.config["languages"],
            "ram_usage": f"{self.config['ram_required']}GB",
            "storage": f"{self.config['storage_required']}GB",
            "optimized_for": self.config["optimized_for"]
        }

# ==================== áƒ˜áƒœáƒ¢áƒ”áƒ áƒ¤áƒ”áƒ˜áƒ¡áƒ”áƒ‘áƒ˜ ====================
class InterfaceManager:
    @staticmethod
    def cli_interface(engine: PrometheusEngine):
        """áƒ™áƒáƒœáƒ¡áƒáƒšáƒ˜áƒ¡ áƒ˜áƒœáƒ¢áƒ”áƒ áƒ¤áƒ”áƒ˜áƒ¡áƒ˜"""
        print("\nğŸ® CLI áƒ áƒ”áƒŸáƒ˜áƒ›áƒ˜ (áƒ’áƒáƒ¡áƒáƒ¡áƒ•áƒšáƒ”áƒšáƒáƒ“: 'áƒ’áƒáƒ›áƒáƒ¡áƒ•áƒšáƒ' áƒáƒœ 'exit')")
        print("="*50)
        
        while True:
            try:
                user_input = input("\nğŸ§‘ > ").strip()
                
                if user_input.lower() in ['áƒ’áƒáƒ›áƒáƒ¡áƒ•áƒšáƒ', 'exit', 'quit', 'áƒ’áƒáƒ›áƒáƒ áƒ—áƒ•áƒ']:
                    print("ğŸ‘‹ áƒœáƒáƒ®áƒ•áƒáƒ›áƒ“áƒ˜áƒ¡!")
                    break
                elif user_input.lower() in ['áƒ˜áƒœáƒ¤áƒ', 'info', 'capabilities']:
                    caps = engine.get_capabilities()
                    print(f"\nğŸ“‹ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ:")
                    for key, value in caps.items():
                        print(f"  {key}: {value}")
                elif user_input.lower() in ['áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ', 'system', 'status']:
                    info = engine.system_info
                    print(f"\nğŸ–¥ï¸ áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ£áƒ áƒ˜ áƒ¡áƒ¢áƒáƒ¢áƒ£áƒ¡áƒ˜:")
                    for key, value in info.items():
                        if 'gb' in key.lower():
                            print(f"  {key}: {value:.1f}GB")
                        else:
                            print(f"  {key}: {value}")
                elif user_input:
                    response = engine.process(user_input)
                    print(f"\nğŸ¤– áƒáƒ áƒáƒ›áƒ”áƒ¢áƒ” > {response}")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ áƒ“áƒ áƒáƒ”áƒ‘áƒ˜áƒ—!")
                break
            except Exception as e:
                print(f"\nâŒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {e}")
    
    @staticmethod
    def web_interface(engine: PrometheusEngine, port: int = 8080):
        """áƒ•áƒ”áƒ‘ áƒ˜áƒœáƒ¢áƒ”áƒ áƒ¤áƒ”áƒ˜áƒ¡áƒ˜áƒ¡ áƒ’áƒáƒ¨áƒ•áƒ”áƒ‘áƒ"""
        print(f"ğŸŒ áƒ•áƒ”áƒ‘ áƒ˜áƒœáƒ¢áƒ”áƒ áƒ¤áƒ”áƒ˜áƒ¡áƒ˜: http://localhost:{port}")
        print("â„¹ï¸ áƒ“áƒáƒ¡áƒáƒ¡áƒ áƒ£áƒšáƒ”áƒ‘áƒšáƒáƒ“: Ctrl+C")
        
        try:
            from http.server import HTTPServer, BaseHTTPRequestHandler
            import threading
            
            class WebHandler(BaseHTTPRequestHandler):
                def do_GET(self):
                    if self.path == '/':
                        self.send_response(200)
                        self.send_header('Content-type', 'text/html; charset=utf-8')
                        self.end_headers()
                        
                        html = f"""
                        <!DOCTYPE html>
                        <html>
                        <head>
                            <meta charset="UTF-8">
                            <title>áƒáƒ áƒáƒ›áƒ”áƒ¢áƒ” AI</title>
                            <style>
                                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                                .container {{ max-width: 800px; margin: auto; }}
                                .prompt-box {{ width: 100%; height: 100px; }}
                                .response {{ background: #f5f5f5; padding: 20px; }}
                            </style>
                        </head>
                        <body>
                            <div class="container">
                                <h1>ğŸ¤– áƒáƒ áƒáƒ›áƒ”áƒ¢áƒ” AI - {engine.config['name']}</h1>
                                <form method="POST">
                                    <textarea name="prompt" class="prompt-box" 
                                              placeholder="áƒ¨áƒ”áƒ˜áƒ§áƒ•áƒáƒœáƒ”áƒ— áƒ—áƒ¥áƒ•áƒ”áƒœáƒ˜ áƒ¨áƒ”áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ..."></textarea><br>
                                    <button type="submit">áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ</button>
                                </form>
                                <div class="response" id="response">
                                    {engine.process("áƒ›áƒáƒ’áƒ”áƒ¡áƒáƒšáƒ›áƒ”áƒ‘áƒ˜áƒ—!")}
                                </div>
                            </div>
                        </body>
                        </html>
                        """
                        self.wfile.write(html.encode('utf-8'))
                
                def do_POST(self):
                    content_length = int(self.headers['Content-Length'])
                    post_data = self.rfile.read(content_length).decode('utf-8')
                    
                    # áƒ›áƒáƒ áƒ¢áƒ˜áƒ•áƒ˜ áƒáƒáƒ¡áƒ¢ áƒ“áƒáƒ¢áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ
                    import urllib.parse
                    data = urllib.parse.parse_qs(post_data)
                    prompt = data.get('prompt', [''])[0]
                    
                    response = engine.process(prompt)
                    
                    self.send_response(200)
                    self.send_header('Content-type', 'text/html; charset=utf-8')
                    self.end_headers()
                    
                    html = f"""
                    <!DOCTYPE html>
                    <html>
                    <body>
                        <div class="response">{response}</div>
                        <script>window.history.back();</script>
                    </body>
                    </html>
                    """
                    self.wfile.write(html.encode('utf-8'))
                
                def log_message(self, format, *args):
                    pass  # áƒšáƒáƒ’áƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ áƒ—áƒ•áƒ
            
            server = HTTPServer(('localhost', port), WebHandler)
            print(f"âœ… áƒ¡áƒ”áƒ áƒ•áƒ”áƒ áƒ˜ áƒ’áƒáƒ¨áƒ•áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ áƒáƒáƒ áƒ¢áƒ–áƒ” {port}")
            server.serve_forever()
            
        except ImportError:
            print("âŒ http.server áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ®áƒ”áƒšáƒ›áƒ˜áƒ¡áƒáƒ¬áƒ•áƒ“áƒáƒ›áƒ˜")
        except Exception as e:
            print(f"âŒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {e}")

# ==================== áƒ›áƒ—áƒáƒ•áƒáƒ áƒ˜ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ ====================
def main():
    parser = argparse.ArgumentParser(
        description="áƒáƒ áƒáƒ›áƒ”áƒ¢áƒ” - áƒ£áƒœáƒ˜áƒ•áƒ”áƒ áƒ¡áƒáƒšáƒ£áƒ áƒ˜ áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜ AI áƒáƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒœáƒ¢áƒ˜",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
áƒ›áƒáƒ’áƒáƒšáƒ˜áƒ—áƒ”áƒ‘áƒ˜:
  %(prog)s                          # áƒáƒ•áƒ¢áƒáƒ›áƒáƒ¢áƒ£áƒ áƒ˜ áƒ áƒ”áƒŸáƒ˜áƒ›áƒ˜
  %(prog)s --cli                    # CLI áƒ˜áƒœáƒ¢áƒ”áƒ áƒ¤áƒ”áƒ˜áƒ¡áƒ˜
  %(prog)s --web                    # áƒ•áƒ”áƒ‘ áƒ˜áƒœáƒ¢áƒ”áƒ áƒ¤áƒ”áƒ˜áƒ¡áƒ˜
  %(prog)s --model tiny             # áƒ™áƒáƒœáƒ™áƒ áƒ”áƒ¢áƒ£áƒšáƒ˜ áƒ›áƒáƒ“áƒ”áƒšáƒ˜
  %(prog)s --prompt "áƒ’áƒáƒ›áƒáƒ áƒ¯áƒáƒ‘áƒ"     # áƒ”áƒ áƒ—áƒ˜ áƒ¨áƒ”áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ
  %(prog)s --batch áƒ¤áƒáƒ˜áƒšáƒ˜.txt        # áƒ¤áƒáƒ˜áƒšáƒ˜áƒ“áƒáƒœ áƒ¬áƒáƒ™áƒ˜áƒ—áƒ®áƒ•áƒ
        """
    )
    
    parser.add_argument('--cli', action='store_true', help='CLI áƒ˜áƒœáƒ¢áƒ”áƒ áƒ¤áƒ”áƒ˜áƒ¡áƒ˜')
    parser.add_argument('--web', action='store_true', help='áƒ•áƒ”áƒ‘ áƒ˜áƒœáƒ¢áƒ”áƒ áƒ¤áƒ”áƒ˜áƒ¡áƒ˜')
    parser.add_argument('--port', type=int, default=8080, help='áƒ•áƒ”áƒ‘ áƒáƒáƒ áƒ¢áƒ˜')
    parser.add_argument('--model', choices=['micro', 'tiny', 'base', 'standard', 'pro', 'ultra', 'auto'], 
                       default='auto', help='áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¢áƒ˜áƒáƒ˜')
    parser.add_argument('--prompt', type=str, help='áƒáƒ˜áƒ áƒ“áƒáƒáƒ˜áƒ áƒ˜ áƒ¨áƒ”áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ')
    parser.add_argument('--batch', type=str, help='áƒ¤áƒáƒ˜áƒšáƒ˜áƒ“áƒáƒœ áƒ¬áƒáƒ™áƒ˜áƒ—áƒ®áƒ•áƒ')
    parser.add_argument('--info', action='store_true', help='áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ£áƒ áƒ˜ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ')
    parser.add_argument('--download', action='store_true', help='áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒáƒ›áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ')
    
    args = parser.parse_args()
    
    try:
        # áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ£áƒ áƒ˜ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ
        if args.info:
            detector = SystemDetector()
            info = detector.get_system_info()
            print(json.dumps(info, indent=2, ensure_ascii=False))
            return
        
        # áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒáƒ›áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ
        if args.download:
            print("ğŸ“¥ áƒ§áƒ•áƒ”áƒšáƒ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ©áƒáƒ›áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ...")
            for model_type in ['micro', 'tiny', 'base', 'standard', 'pro', 'ultra']:
                ModelManager.download_model(model_type)
            return
        
        # AI áƒ«áƒ áƒáƒ•áƒ˜áƒ¡ áƒ¨áƒ”áƒ¥áƒ›áƒœáƒ
        print("ğŸ” áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ˜áƒ¡ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜...")
        engine = PrometheusEngine(model_type=args.model)
        
        # áƒáƒ˜áƒ áƒ“áƒáƒáƒ˜áƒ áƒ˜ áƒ¨áƒ”áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ
        if args.prompt:
            response = engine.process(args.prompt)
            print(f"\nğŸ¤– áƒáƒáƒ¡áƒ£áƒ®áƒ˜:\n{response}\n")
            return
        
        # áƒ¤áƒáƒ˜áƒšáƒ˜áƒ“áƒáƒœ áƒ¬áƒáƒ™áƒ˜áƒ—áƒ®áƒ•áƒ
        if args.batch:
            try:
                with open(args.batch, 'r', encoding='utf-8') as f:
                    prompts = [line.strip() for line in f if line.strip()]
                
                print(f"ğŸ“– áƒ¤áƒáƒ˜áƒšáƒ˜áƒ“áƒáƒœ áƒ¬áƒáƒ™áƒ˜áƒ—áƒ®áƒ•áƒ: {len(prompts)} áƒ¨áƒ”áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ")
                responses = engine.batch_process(prompts)
                
                for i, (prompt, response) in enumerate(zip(prompts, responses), 1):
                    print(f"\n{i}. â“ {prompt}")
                    print(f"   ğŸ¤– {response}")
                    
            except FileNotFoundError:
                print(f"âŒ áƒ¤áƒáƒ˜áƒšáƒ˜ '{args.batch}' áƒáƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ")
            return
        
        # áƒ•áƒ”áƒ‘ áƒ˜áƒœáƒ¢áƒ”áƒ áƒ¤áƒ”áƒ˜áƒ¡áƒ˜
        if args.web:
            InterfaceManager.web_interface(engine, args.port)
            return
        
        # CLI áƒ˜áƒœáƒ¢áƒ”áƒ áƒ¤áƒ”áƒ˜áƒ¡áƒ˜ (áƒ¡áƒ¢áƒáƒœáƒ“áƒáƒ áƒ¢áƒ£áƒšáƒ˜)
        InterfaceManager.cli_interface(engine)
        
    except SystemError as e:
        print(f"âŒ {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ áƒ“áƒ áƒáƒ”áƒ‘áƒ˜áƒ—!")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ áƒ™áƒ áƒ˜áƒ¢áƒ˜áƒ™áƒ£áƒšáƒ˜ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {e}")
        sys.exit(1)

# ==================== áƒ’áƒáƒ¨áƒ•áƒ”áƒ‘áƒ ====================
if __name__ == "__main__":
    # áƒ•áƒáƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ— áƒ“áƒáƒ›áƒáƒ™áƒ˜áƒ“áƒ”áƒ‘áƒ£áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ¡
    required_packages = ['psutil', 'torch']
    
    print("ğŸ”§ áƒáƒ áƒáƒ›áƒ”áƒ¢áƒ” AI - áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ...")
    
    # áƒ•áƒáƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ— Python áƒ•áƒ”áƒ áƒ¡áƒ˜áƒáƒ¡
    if sys.version_info < (3, 8):
        print("âŒ áƒ¡áƒáƒ­áƒ˜áƒ áƒáƒ Python 3.8 áƒáƒœ áƒ£áƒ¤áƒ áƒ áƒ›áƒáƒ¦áƒáƒšáƒ˜")
        sys.exit(1)
    
    main()
